\documentclass[11pt, a4paper]{article}

\usepackage[margin=2cm]{geometry}

\usepackage{setspace}
\onehalfspacing

\usepackage{lineno}
\linenumbers

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{amsmath}
\usepackage[labelfont=bf]{caption}

% command to create supplementary figures
\newcommand{\beginsupplement}{
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

% command to get word count from Write_up.sum
\newcommand{\wordcount}{
	\input{Write_up.sum}
	}

\begin{document}
%\setlength{\parindent}{0pt} % sets indent to 0
\setlength{\parskip}{6pt}

\title{The importance of analysis design when fitting mathematical models to thermal response curves}
\author{Elin Falla\\Imperial College London
\\ Word count: \wordcount}
\date{}
\maketitle
\pagebreak

\section*{Abstract}
\textbf{With the onset of climate change, it is more important than ever that we understand species' responses to temperature changes. Fitting mathematical models to thermal response curves (TRCs) is a popular method for analysing and predicting the response of individual species/processes to a warming climate. This study shows the importance of careful analysis design in these cases, as non-biological factors can drastically influence the conclusions drawn from model fitting. I fit linear and non-linear models from the literature to 903 TRCs and analysed them using different model selection criteria, as well as analysing the effect of sample size and data quality on the results. I found that use of AIC$_c$ rather than AIC or BIC drastically changes the optimal model, due to the large number of small datasets. The optimal model is also affected by the shapes of TRCs considered: the non-linear models performed best on unimodal datasets. Overall, this study shows the importance of exploring your data and the models you fit, in order to design an impactful analysis that produces accurate predictions.}

\hrulefill

\section{Introduction}
Temperature is among the most important environmental factors, as all biological processes respond to it \cite{yanEquationModellingTemperature1999}. There are plentiful studies on the effects of temperature on different biological processes \cite{adamsModelFitBiological2017, krenekThermalPerformanceCurves2011, damosTemperatureDrivenModelsInsect2011, zwieteringModelingBacterialGrowth1991}, as well as meta-analyses across ecological and physiological traits \cite{dellSystematicVariationTemperature2011}, and for good reason. Climate change is changing the thermal landscape \cite{pachauriClimateChange20072008} and understanding species' responses is critical to mitigating its effects.

Among these important processes are respiration and photosynthesis; there are many studies on their responses to temperature \cite{gutowMarineMesoherbivoreConsumption2016, sageTemperatureResponseC32007, caronEffectTemperatureGrowth1986, wittmannTemperatureDependencyBark2007, hanckeTemperatureEffectsMicroalgal2008}. For single species analysis, thermal response curves (TRCs) are often used, that measure trait values across a range of temperature values \cite{dellSystematicVariationTemperature2011}. A large variety of mathematical models, both phenomenological and mechanistic, have been proposed to fit TRCs. Such models aim to "depict biological processes in simplified and general ways that provide insight into factors that are responsible for observed patterns" \cite{johnsonModelSelectionEcology2004}.

In recent years, model selection methods have gained traction as a way to assess model fit, in which competing models are simultaneously confronted with data in order to determine the best one  \cite{johnsonModelSelectionEcology2004}. There are many model selection criteria, each with their own set of assumptions. However, there can be confusion around which set of criteria are best, and in addition model selection is often affected by non-biological factors, such as sample size and aspects of data quality \cite{burnhamModelSelectionMultimodel2002}.

As climate change is such a pressing issue, it is critical that the predictions made from TRCs are as accurate as possible, and that the optimal models are chosen correctly. The aim of my study is to vary analysis design when fitting models to respiration and photosynthesis TRCs, in order to assess the effects that it can have on which model is considered optimal, and hence the conclusions drawn. I will vary the selection criteria used, as well as analysing the effects of sample size and data quality and type. I will also compare the photosynthesis and respiration TRCs, to see if and how non-biological differences in their data can affect the best model for each response type.



%- This study: comparing selection criteria to see how it changes the outcome and comparing model selection for respiration versus photosynthesis rates - is the best model different?

%- aim isn't to find the optimal model for thermal response curves (only fitting 5) but to investigate the effects of non-biological factors such as the data quality (sample size, response curve shape) and chosen model selection criteria on the optimal model. 

%- importance of vetting and preparing your data

%\textbf{**NOTES**}
%"we do not accept that there is a simple "true model" in biological sciences. modelling is an exercise in the approximation of the explainable information in the empirical data, in the context of the data being a sample from some well defined population or process." (textbook)
	
	%- "Temperature is among the most important environmental factors that control plant development, growth and yield. All biological processes respond to temperature, and all responses can be summarized in terms of three cardinal temperatures, namely the base or minimum (Tmin), the optimum (Topt), and the maximum (Tmax) temperatures." (yan and hunt 1999)
	
	%- eg. "Climate has a profound effect on the distribution and abundance of invertebrates such as insects, and the mathematical description of the climatic influence on insect development has been of considerable interest among entomologists. " (damos 2012)
	
	%- for temp range of photosynthesis see sage 2007
		
\section{Materials and Methods}
	
	\subsection{The data}
	The data is a subset of the "BioTraits" database, containing 903 TRCs (referred to as 'datasets') for respiration and photosynthesis. The data is compiled from many sources so does not have consistent rate units, but all temperatures are in degrees Celcius.

%	As the rate value is often in comparison to a reference rate, there were cases of negative values for the rate variable. For each ID containing at least one negative rate value, all values were shifted upwards so the smallest rate value was 0.
	
	I classified the datasets according to the type of response curve they produced when fitted by ordinary least squares (OLS) regression to a quadratic function. The classifications were 'rising', 'falling', 'unimodal' and 'non-typical'. The latter was used for curves that didn't fall into the previous categories and/or had a fit with $R^2$ value $<$ 0.5. Figure~\ref{fig:curves} shows examples for each curve classification.
	
	\subsection{The models}
	5 models were fitted to the data: 2 linear and 3 non-linear. All are phenomenological in nature but the non-linear models have biologically-relevant parameters whereas the linear do not. 
	
	\subsubsection*{Linear models}	
	The two linear models were the quadratic and cubic polynomial equations (Equations~\ref{eq:quad} and \ref{eq:cubic}). In both cases $R$ represents the rate (of respiration or photosynthesis) and $T$ represents the temperature. The quadratic equation has 3 parameters ($a$, $b$ and $c$) and the cubic has 4 ($a$, $b$, $c$ and $d$), but as these models are purely phenomenological, in both cases these parameters hold no biological significance.
	
	\begin{equation} \label{eq:quad}
	R = a + bT + cT^2
	\end{equation}
	\begin{equation} \label{eq:cubic}
	R = a + bT + cT^2 + dT^3
	\end{equation}
	
	\subsubsection*{Non-linear models}
	Three non-linear models were fitted to the data: two versions of the Briere model \cite{briereNovelRateModel1999}, and the Yan and Hunt model \cite{yanEquationModellingTemperature1999}.
	
	The two Briere models are very similar. Briere-1 contains parameters $B_0$, $T_0$ and $T_m$. $B_0$ is a constant with no biological meaning, whereas $T_0$ and $T_m$ are the minimum and maximum temperatures at which the process (photosynthesis or respiration) can occur. At temperatures above $T_m$ and below $T_0$, the rate is 0. Briere-2 is the same, except it has an extra parameter $m$ that lends more flexibility. (In Briere-1, $m = 2$.)
		
	\begin{equation} \label{eq:briere}
	R = \left\{
			\begin{array}{ll}
			0 & \quad T \leq T_0 \\
            B_0 T (T-T_0) \sqrt{T_m - T} & \quad T_0 \leq T \leq T_m \\
            0 & \quad T \geq T_m
			\end{array}
	\right.
	\end{equation}
	
	\begin{equation} \label{eq:briere2}
	R = \left\{
			\begin{array}{ll}
			0 & \quad T \leq T_0 \\
            B_0 T (T-T_0) (T_m-T)^\frac{1}{m} & \quad T_0 \leq T \leq T_m \\
            0 & \quad T \geq T_m
			\end{array}
	\right.
	\end{equation}

	The final model is the Yan and Hunt model (Equation~\ref{eq:yanhunt}), another phenomenological model with 4 parameters. $R_{max}$ is the maximum rate of respiration/photosynthesis, and $T_{opt}$ is the temperature at which this occurs. $T_{max}$ and $T_{min}$ are equivalent to $T_0$ and $T_m$ in the Briere-1 and Briere-2 models. Like for the Briere models, the rate $R$ will be 0 when the temperature is larger than $T_{max}$ or smaller than $T_{min}$.
	
	\begin{equation} \label{eq:yanhunt}
	R = R_{max} 
		\left(
			\frac{T_{max} - T}{T_{max} - T_{opt}}
		\right) 
		\left(
			\frac{T - T_{min}}{T_{opt} - T_{min}}
		\right)
			^\frac{T_{opt} - T_{min}}{T_{max} - T_{opt}}
	\end{equation}

	\subsection{Model fitting and selection}
	\subsubsection*{Computing tools}
	All analyses were performed in R (version 3.4.2) using RStudio. R was used due to its many useful in-built functions and statistical packages. It also has excellent data visualisation tools (ggplot2 package). I did not mix use of R and Python. This created coherency across my scripts, and allowed me to use functions and variables from one script in another without having to import them. To run all the scripts together, I used bash, due to its ease of use.
	
	\subsubsection*{Model fitting}
	The quadratic and cubic models were fit using OLS regression, and the non-linear models were fit using non-linear OLS regression, using the Levenberg-Marquardt type fitting algorithm. I sampled the starting values for all parameters of all non-linear models to achieve the best possible fit.
	
	\subsubsection*{Model selection}
		I used three model selection criteria, in order to compare the results of each. All provide a measure for the quality of fit of a particular model to a dataset \cite{sakamotoAkaikeInformationCriterion1988}. I used Akaike Information Criterion (AIC), the second-order corrected AIC (AIC$_c$), and Bayesian Information Criterion (BIC). AIC assumes there is no "true model" for the data, and ranks models according to their closeness to a hypothetical "true" model, whereas BIC assumes that there is a "true model" and that it's within the model set \cite{burnhamModelSelectionMultimodel2002}. AIC$_c$ is a version of AIC that not only compensates for bias in fit with model complexity (as AIC does \cite{akaikeNewLookStatistical1974}) but also compensates for bias with small sample sizes \cite{hurvichRegressionTimeSeries1989}. For each dataset, the best model is the one with the smallest AIC/BIC/AIC$_c$, calculated according to Equations~\ref{eq:AIC}-\ref{eq:AICc}. These equations lay out their implementation in my code (implicitly in the case of AIC and BIC), where $n$ refers to the sample size, $K$ to the number of free parameters in the model, and $RSS$ to the residual sums of squares of the model fit. The number of times a model was deemed the best fit by the respective model criteria (henceforth described as 'wins') was compared across models. 
	
	\begin{equation} \label{eq:AIC}
	AIC = n + 2 + n\ln
		\left(
			\frac{2\pi}{n}								\right) 
	+ n\ln(RSS) + 2K
	\end{equation}
	
	\begin{equation} \label{eq:BIC}
	BIC = n + 2 + n\ln
		\left(
			\frac{2\pi}{n}								\right) 
	+ n\ln(RSS) + K\ln(n)
	\end{equation}
	
	\begin{equation} \label{eq:AICc}
	AIC_c = n + 2 + n\ln
		\left(
			\frac{2\pi}{n}								\right) 
	+ n\ln(RSS) + 2K 
		\left(
			\frac{n}{n - K - 1}
		\right)
	\end{equation}

	As an additional measure of model success, Akaike weights were calculated from the AIC and AIC$_c$ values. Weights provide the relative weight of evidence for each model for each dataset, and sum to 1 \cite{johnsonModelSelectionEcology2004}. Equation~\ref{eg:weights} shows how $W_i$, the weight of model $i$, was calculated from AIC for each dataset $y$, where $L(i | y)$ is the likelihood of model $i$ given the data $y$, and $R$ is the total number of models. The same equation was used to calculate Akaike weights from AIC$_c$, except with $AIC_i - AIC_{min}$ substituted for $AICc_i - AICc_{min}$. The distribution of Akaike weights for each model was analysed, with the median weight acting as a proxy for the overall goodness of fit of a model across all datasets. I chose the median over the mean as it better represented the typical weight for a particular model, as there were often outliers at very high weight values, skewing the mean.
	
	\begin{align} \label{eg:weights}
	W_i &= \frac{L(i | y)}{\sum_{j = i}^{R}L(j | y) }
	& L(i | y) &= exp(-\frac{1}{2} (AIC_i - AIC_{min})) 
	\end{align}
	
	
\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Shapes_of_curves.pdf}
	\caption{\label{fig:curves} \textbf{Example TRC fits.} Examples of the different curve classifications of the datasets with all models fitted to them. 'Rate' refers to the rate of either photosynthesis or respiration, units vary. (a) Unimodal, dataset ID = 118 (b) Rising, ID = 22 (c) Falling, ID = 52 (d) Non-typical, ID = 99}
\end{figure}	

\begin{figure} [H]
	\centering
	\includegraphics[width = \textwidth]{../Figures/Curve_type_wins.pdf}
	\caption{\label{fig:wins_plot} \textbf{Number of wins for each model under different selection criteria.} Number of wins is split into wins per curve classification for each model for (a) AIC$_c$ (b) AIC (c) BIC. Number of 'wins' refers to the number of datasets where a particular model was deemed the best fit by the model selection criteria. Numbers over the bars equate to the total number of wins for each model. A linear model won under all selection criteria, partly due to them winning a large proportion of non-unimodal datasets.}	
\end{figure}

\section{Results}

\subsection{The optimal model according to the selection criteria}
The optimal model for the data varied depending on the model selection criteria, but for all, a linear model had the best overall fit, both according to number of 'wins' and median Akaike weight (in the case of AIC and AIC$_c$).

\subsubsection*{The model with the most wins}
Using both the AIC and BIC selection criteria, the cubic model had the most wins (Figure~\ref{fig:wins_plot}). However, using AIC$_c$ the quadratic model had the most wins. The best non-linear model according to AIC and BIC was the Yan and Hunt model, whereas it was the Briere-1 model according to AIC$_c$.



\subsubsection*{The model with the highest median Akaike weight}
The median Akaike weight for each model gave the same result as the number of wins for both AIC and AIC$_c$, except in the case of the Yan and Hunt and quadratic models under AIC (Figure~\ref{fig:weights}a and b). The quadratic model had more wins than Yan and Hunt, but a far lower median Akaike weight. This suggests that quadratic model either wins or doesn't fit well, whereas the Yan and Hunt model is more consistently a good fit, even if it doesn't win. Across all models, for both AIC and AIC$_c$, the range of Akaike weights for each model was very large, showing the diversity of the datasets.

\subsection{The effect of the curve classification}
All models visually fit well on the expected TRC: a negative parabola with a sharp decline at higher temperatures (unimodal) (Figure~\ref{fig:curves}a). However, not all datasets took this shape, as shown by Figure~\ref{fig:curves}. Cases of rising responses were very common (Figure~\ref{fig:curves}b), and there were also a few cases of falling responses (Figure~\ref{fig:curves}c). This could be due to an insufficient range of temperatures being tested. However, there were also a significant number of datasets whose data did not fit the expected TRC at all, classed as non-typical (Figure~\ref{fig:curves}d).

As the non-linear models have a far less flexible shape compared to the linear quadratic and cubic equations, they were much less likely to win against the linear models for non-unimodal response curves, and sometimes didn't fit well at all. This is evident, as the linear models (cubic and quadratic) won a far higher proportion of non-unimodal (falling, rising and non-typical) datasets than the non-linear models, especially for AIC and BIC (Figure~\ref{fig:wins_plot}). When comparing only unimodal response curves, the Yan and Hunt model had more wins than quadratic under both AIC and BIC whereas the reverse is true when non-unimodal response curves are considered.


\subsection{The effect of sample size}
The result from AIC$_c$ was very different to AIC and BIC in terms of the optimal models chosen (Figures~\ref{fig:wins_plot} and \ref{fig:weights}). Under AIC$_c$, the quadratic and Briere-1 models had far more wins than the other models, and also were the only models with wins at very small sample sizes (5 - 6 data points) 

\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Weights_boxplot.pdf}
	\includegraphics[width=\textwidth]{../Figures/Weights_boxplot_SS.pdf}
	\caption{\label{fig:weights} \textbf{Distribution of Akaike weights with and without small sample sizes.} Boxplot showing distribution of Akaike weights across the datasets for each model using different selection criteria (a) AIC for all datasets (b) AIC$_c$ for all datasets (c) AIC for datasets with sample size $>$ 10 (d) AIC$_c$ for datasets with sample size $>$ 10. AIC$_c$ results very different to AIC with all datasets considered, but much more similar with small datasets removed.}
\end{figure}

\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Sample_size_per_model.pdf}
	\caption{\label{fig:sample_size_line} \textbf{Proportion of wins across dataset sample sizes.} The proportion of all wins with each sample size (for sample size $<$ 30), for each model and selection criteria: (a) AIC$_c$ (b) AIC (c) BIC. 'Wins' refers to the number of datasets where a particular model was deemed the best fit by the model selection criteria. Briere-1 and quadratic have a high proportion of wins at very small sample sizes using AIC$_c$. Line of best fit determined using local polynomial regression fitting (loess() function).}
\end{figure}
\bigskip

(Figure~\ref{fig:sample_size_line}a). This suggests that at these small sample sizes, the number of parameters of the model plays a very large part in model selection. The quadratic and Briere-1 models both have three parameters (Equations~\ref{eq:quad} and \ref{eq:briere} respectively), whereas all of the other models (Briere-2, cubic, Yan and Hunt) have four parameters. Therefore, the selection of the quadratic model under AIC$_c$ has more to do with the large number of datasets with small sample sizes than it visually fitting the data points better. The other models were only able to win for datasets with more data points. AIC$_c$ is a small-sample corrected version of AIC, with a focus on preventing over-fitting by penalising models with more parameters at smaller sample sizes, so this result makes sense.

This is further corroborated by the fact that when datasets with the smallest sample sizes (5 - 10 data points) weren't considered, the median Akaike weight by model according to AIC$_c$ looked much more like the result for AIC (Figure~\ref{fig:weights}c and d). In this case the Yan and Hunt model and cubic appear on par as the optimal models, rather than the quadratic. 

The optimal model selected by AIC is also affected by the elimination of small datasets: Akaike weight distribution of all the non-linear models shifted up (towards 1), and the reverse is true of the linear models, especially the cubic. This means the winning model changes from the cubic to Yan and Hunt model (although only by a very small margin) (Figure~\ref{fig:weights}a versus \ref{fig:weights}c). AIC$_c$ therefore isn't the only model selection criteria affected by sample size. The linear models were negatively affected as they have the highest proportion of wins at very small sample sizes (Figure~\ref{fig:sample_size_line}b). This can be partly explained by the fact that a high proportion of the responses classed as 'falling', which the non-linear models often didn't fit well, had very small sample sizes (Figure~\ref{fig:sample_size_per_curve}b). However, as there were very few response curves classed as falling (Figure~\ref{fig:sample_size_per_curve}a), this can't entirely explain the result. The proportion of rising and non-typical responses with small sample sizes don't differ from that of unimodal responses (Figure~\ref{fig:sample_size_per_curve}b). It's possible that these non-linear models just don't fit as well at small sample sizes as they do at larger sample sizes.

\subsection{The optimal model for respiration versus photosynthesis}

When the datasets are split into response curves for respiration and photosynthesis, the optimal model doesn't change. However, the linear models have a higher proportion of wins for respiration response curves than for photosynthesis response curves, across all model selection criteria (AIC, BIC and AIC$_c$) (Figure~\ref{fig:photo_resp_model}). Further analysis reveals that this is likely due to differences between their datasets: the photosynthesis datasets have a far higher proportion of unimodal response curves than the respiration datasets. The respiration TRCs mainly consist of rising response curves (Figure~\ref{fig:photo_resp_curve_type}). In addition, a higher proportion of the respiration datasets have very small sample sizes than the photosynthesis datasets (Figure~\ref{fig:photo_resp_ss}). Low sample size and non-unimodal response curves both cause reduced goodness of fit for non-linear models across all model selection criteria, as discussed earlier.


\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Sample_size_per_curve.pdf}
	\caption{\label{fig:sample_size_per_curve} \textbf{Distribution of sample sizes across each curve classification.} (a) Number of datasets per sample size per curve classification. (b) Datasets per sample size as a proportion of total datasets for each curve classification. Line of best fit determined using local polynomial regression fitting (loess() function). Proportion of 'falling' curves at low sample sizes is disproportionately high.}
\end{figure}

\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Photo_resp_model_wins.pdf}
	\caption{\label{fig:photo_resp_model} \textbf{Proportion of wins for photosynthesis versus respiration TRCs.} Proportion of photosynthesis and respiration TRCs won by each model, for each selection criteria: (a) AIC$_c$ (b) AIC (c) BIC. Respiration TRCs have a higher proportion of wins by linear models under all selection criteria.}
\end{figure}


\section{Discussion}

This study investigated the effect that non-biological considerations, such as sample size, data quality and selection criteria, can have on the optimal model for a  series of datasets. I found that all these factors affected the optimal model to some extent.

As expected, using different model selection criteria significantly impacts the optimal model, especially in the case of AIC versus AIC$_c$. When the number of parameters ($K$) exceeds $n/40$ (such as at small sample sizes), AIC$_c$ should be used \cite{johnsonModelSelectionEcology2004}. The principle of parsimony aims to find a model with "the smallest possible number of parameters for adequate representation of the data" \cite{burnhamModelSelectionMultimodel2002}, causing a trade-off between bias and variance, synonymous with a trade-off between overfitting and underfitting \cite{burnhamModelSelectionMultimodel2002}. In AIC, the last term of the equation, "$+2K$" (Equation~\ref{eq:AIC}) increases with increasing model dimension (as $K$ is the number of parameters), preventing overfitting by penalising higher dimensional models. However, this isn't sufficient at very small sample sizes ($n$), as is the case for the data I've analysed, where the most common values of $n$ are 5 and 6, and $K$ is 3 or 4 for all models. AIC$_c$ (Equation~\ref{eq:AICc}) is the equivalent of AIC + $\frac{2K(K+1)}{n-K-1}$. This extra term more heavily penalises higher dimensional models at smaller sample sizes, tending towards AIC as sample size increases. This is clear from my results, as on the smallest datasets, the 3 parameter models (quadratic and Briere-1) were exclusively chosen.

Using BIC gave a similar result to AIC, suggesting it also doesn't sufficiently penalise higher dimensional models. This makes sense given the architecture of BIC. BIC is based on the assumption that there is a "true" model for the data, and that it's within the given model set. As such, the "best" model doesn't vary with sample size \cite{burnhamModelSelectionMultimodel2002, schwarzEstimatingDimensionModel1978}. By contrast, as AIC is built on the assumption that no "true" model exists, it selects the best model for a given sample size \cite{burnhamModelSelectionMultimodel2002}. Therefore, AIC (or AIC$_c$ in this case) is likely a better model selection criteria than BIC, given that I have fitted only five models and the chance one of them is the "true" model (if such a thing exists) is practically non-existent. AIC and BIC have also been compared empirically, such as by Umbach and Wilcox \cite{umbachTechniqueMeasuringEpidemiologically1996}, who found for a sample size up to 100,000 AIC performed better than BIC. As I am working with sample sizes a fraction of this amount, this reinforces that AIC$_c$ is the correct method to use.

There were also differences when comparing the models using Akaike weights versus the number of wins, such as in the case of the Yan and Hunt and quadratic models using AIC (see Results). The distribution of weights gives an idea of the average goodness of fit across the datasets whereas the number of wins shows best for each dataset, regardless of how good the other models were in comparison, losing important data. It's also important to consider that both these measures show which models are the best of the five fitted, but not necessarily the absolute best for these TRCs. Selection criteria can only select the best model from the candidate set \cite{johnsonModelSelectionEcology2004}.

This study also shows the importance of knowing your data and refining it if necessary. With all 903 datasets considered, AIC$_c$ selected the quadratic model as the optimal model, followed by Briere-1. The high number of datasets with extremely small sample sizes caused this result; on a dataset with only 5-6 data points, it is very unlikely a model with $K=4$ will be selected as optimal. When those datasets with the smallest sample sizes were removed, the optimal models changed dramatically. This demonstrates the importance of comparing your data and the models you propose to fit, and possibly not including extremely small datasets.

With the smallest datasets removed, the Yan and Hunt model had the highest median Akaike weight of the non-linear models. It had an extremely similar median weight to the cubic model, but linear models aren't as useful, as their parameters have no biological meaning. The Yan and Hunt model has found success in the literature \cite{adamsModelFitBiological2017, vanderheideSimpleEquationDescribing2006}. For example, Adams et al. \cite{adamsModelFitBiological2017} determined it was the best model to describe tropical seagrass photosynthesis TRCs, as it had one of the best fits and biologically meaningful parameters. However, Briere-1 could also be considered the best non-linear model, as it had the best fit (after the quadratic model) when all datasets were included. The choice of whether to include datasets with small sample sizes depends on the circumstances of the analysis and is down to the discretion of the researcher. These decisions are important, as they clearly have major impacts on the conclusions drawn.

The shape of the response curve is another important consideration. The models I've used are all phenomenological and designed to fit a full unimodal TRC \cite{yanEquationModellingTemperature1999, briereNovelRateModel1999}, which could explain why they performed worse on non-unimodal curves. Some non-linear models have adjustments for fitting rising/falling data, such as the Boltzman-Arrhenius \cite{dellSystematicVariationTemperature2011} or Schoolfield  \cite{schoolfieldNonlinearRegressionBiological1981} models, which should be taken into account when designing an analysis. However, in general, it is advisable to measure thermal response over the full range of temperatures, as this has both been essential for making predictions about the effects of climate change  \cite{hueyWhyTropicalForest2009}, and ensures optimal fits \cite{dellSystematicVariationTemperature2011}.

The culmination of these considerations can be seen when comparing the photosynthesis and respiration TRCs; a great deal of their differences in optimal models can be explained by differing curve types and sample sizes rather than biological differences. However, further research is needed into whether some of the variation is caused by biology, as respiration has been predicted to increase faster with rising temperature than photosynthesis due to the differing temperature-dependency of their key enzymes \cite{gutowMarineMesoherbivoreConsumption2016, allenLinkingGlobalCarbon2005}.

There are also other areas where further research would be beneficial. Firstly, fitting a larger number and wider variety of model types would benefit the analysis, for example allowing a comparison of mechanistic and phenomenological models. In addition, my analysis showed evidence that the non-linear models fit better at larger sample sizes, which would benefit from further study to see if this is a true effect. A potential weakness of my study is that I didn't use the rule of thumb for model selection, that states if two models have fits within 2 of each other, both have substantial support and are optimal models for the dataset \cite{johnsonModelSelectionEcology2004, burnhamModelSelectionMultimodel2002}. This would be useful to include in a future analysis, especially as the models often won by a small margin. However, there is some assertion that when sample sizes are very small, these guidelines cannot be expected to hold \cite{burnhamModelSelectionMultimodel2002}, so I think my analysis is still valid without this consideration. Finally, the linear models doing as well as or better than the non-linear in many cases could suggest further optimisation of the fitting of the latter is needed. However, this could also be due to small temperature ranges in many datasets \cite{dellSystematicVariationTemperature2011}, another area that would benefit from further study.

In conclusion, this study highlights the effects that analysis design can have on the conclusions drawn from an investigation, and therefore its predictions. It is important to carefully select the model selection criteria, models, and data subset used. With the onset of climate change, this is especially important in studies on the effects of temperature on species, as making accurate predictions will be vital to mitigating its effects.

\bibliographystyle{ieeetr}
\bibliography{Bibliography_main.bib}

\newpage
\section{Supplementary information}

\beginsupplement % labels figures as S1, S2 etc.

\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Photo_resp_curve_type.pdf}
	\caption{\label{fig:photo_resp_curve_type} \textbf{Curve classification for photosynthesis versus respiration TRCs.} Number of respiration and photosynthesis datasets, divided into curve classifications. Photosynthesis TRCs have a far higher proportion of unimodal curves than respiration TRCs.}
\end{figure}

\begin{figure} [H]
	\includegraphics[width=\textwidth]{../Figures/Photo_resp_sample_size.pdf}
	\caption{\label{fig:photo_resp_ss} \textbf{Distribution of sample sizes for respiration versus photosynthesis TRCs.} Proportion of all datasets with each sample size (for sample size $<$ 50), for each response type (respiration and photosynthesis). A higher proportion of respiration datasets have small sample sizes than photosynthesis.}
\end{figure}

\end{document}